1/4

1. split-apply-combine method to subset weather Data
2. use dataset[,x:y] to subset weather columns in a loop or function

1/8

1. Worked on G2F weather dataset; subset data and accounting for missing obs
2. worked with Austin G on benchmark tool; relayed possible changes for Premium to Silvia
3. Looking at TACC tutorials; trying to build best practice for using R on Stampede2
    a. start with Data Analysis with R on HPC
        i. use names() then attach()
        ii. update tutorials for Stampede2; old stampede1 links and pkgs that are not supported in lated R version
        iii. are we using all of the local cores on our machines
        iv. need to link the profiling tutorial and info on clustering to lab somehow
        v. how to launch R in Stampede2?
4. What are we trying to do with the known-truth simulations for Premium?
5. understanding the way we use Bayesian Clustering and profiling
    a. a profile is the similarities discovered between the covariates
    b. clusters are made and randomly assigned the profiles that were discovered
    c. many iterations of clusters are created using the discovered profiles
    d. we use the clusters with regression to predict future covariates
    e.
6. need to read more about MCMC
7. build solid routine for work

1/9

1. Goals
    a. Run Premium with test code on Stampede2
    b. Complete Slurm Tutorial - Cornell
    c. Learn about SparkR pkg

2. Worked On
    a. cornell Stampede2 tutorial
    b. tested multicore perf on Stampede2
    c. setup RStudio thru TACC Vis Portal for Premium
    d. profiled how long Premium takes for simulation and on Subset_of_Final_Input data
        using local machine and Stampede2 and compared results
    e. reread the Stampede2 User Guide
    f. building a test script to benchmark Premium
    g. using SBATCH/Slurm with R
    h. first chapter of sparkly tutorial
    i. read first set of slides on HDFS from TACC
    j. completed Cornell MapReduce tutorial

3. General Notes
    a. Add Cornell tutorials to Slack
        i. Stampede2 - https://cvw.cac.cornell.edu/Environment/default
    b. Was practicing poor citizendship using login nodes to run R
        i. use the idev command and access a compute node

1/10

1. Goals
    a. Access G2F data on Wrangler using Spark
    b. Use cleaned weather data and run thru Premium
    c. Setup Spark mini-cluster
    d. Read best practice for data transfer
    e. iRods tutorial
    f. CyVerse Youtube videos

2. Worked On
    a. wrangled data and submitted job for hybrids
    b. downloaded spark and tried to run SparkR
            i. had dependecy issues
            ii. had to revert to older version of java
            iii. compiled version of SparkR but only running in terminal
    c. ran through tutorial on sparklyr
    d. practiced moving data with icommands and agave



3. General Notes
    a. Write a report of what got done tomorrow
    b. look up prezzie on gmo's
    c. icommands seems easier than agave

1/11

1. Goals
    a. create an example for students to see different data transfer options
    b. submit hybrid job with remora

2. Worked On
    a. setup remora to run on hybrid job and track perf stats
    b. installed Hive and setup for Spark$
    c. setup Spark mini cluster and test with g2f dataset using SparkR
    d. many problems getting local SparkR to work properly
        i. hive not setup right
        ii. hadoop not compiling native lib
    e. using Kitematic and Docker containers for SparkR

3. General Notes
    a. create an example for transerring files between datastore and TACC
